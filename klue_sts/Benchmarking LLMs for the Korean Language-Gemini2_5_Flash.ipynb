{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ee3a31-00c5-4122-b432-2928037d2a0a",
   "metadata": {},
   "source": [
    "# KLUE-STS with Gemini 2.5 Flash\n",
    "- Created: 2025-06-26 (Thu)\n",
    "- Updated: 2025-06-26 (Thu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25af0c-ee47-4b5e-bf36-1f02f5fc2a93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Environment Set-up\n",
    "- scikit-learn is used to evaluate the performance report, e.g. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6a2384f-d4fe-4386-a673-583931266080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4152f71-7db3-49a2-a5da-684238e7d7d5",
   "metadata": {},
   "source": [
    "## 2. Vertex AI Gemini Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d145d4d5-8a3d-44ed-a7b9-eda85c42beb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3012f1-75aa-4a09-bb1a-9cedcbb13bcc",
   "metadata": {},
   "source": [
    "### Restart kernel after installs so that your environment can access the new packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dff5d11c-0916-4824-9e68-c17a7e75976f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf111c4-369c-42a3-91ef-bcd08a8ea179",
   "metadata": {},
   "source": [
    "- Skip running the following cell if you use Vertex AI Workbench.\n",
    "- Run it only for Colab Enterprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d8c3-d62d-49e0-b7d9-f6ef8003b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5f77b9-175b-4ae8-8d4f-9a67392254c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID=vertex-workbench-notebook\n",
      "LOCATION=us-central1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, Image, Markdown, display\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")\n",
    "\n",
    "print(f\"PROJECT_ID={PROJECT_ID}\")\n",
    "print(f\"LOCATION={LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078673d7-deab-40d8-8401-0f3119450458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97dfe17-f7de-4f07-a3c1-2ca7fd467cce",
   "metadata": {},
   "source": [
    "## 3. Load the dataset\n",
    "The total number of samples in KLUE-STS validation is known to be \"519\". \n",
    "However this number will be verified and saved to a variable `total_num_of_samples`.\n",
    "\n",
    "Without this number, it will be difficult to judge how much progress is made with the entire dataset and the user should wait blindlessly.\n",
    "It's a better practice to show the progress like:\n",
    "\n",
    "```bash\n",
    "Processing the full validation dataset...\n",
    "Evaluating gemini-2.5-flash with the entire dataset of (519 samples):   8%|▊         | 41/519 [05:09<54:40,  6.86s/it]  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38df58ad-2936-432d-980d-ec3847bdd3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples in KLUE-STS validation is: 519\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Set benchmark dataset and task variables\n",
    "benchmark_dataset = \"klue\"\n",
    "benchmark_task    = \"sts\"\n",
    "\n",
    "# Load the dataset *without* streaming to get its length\n",
    "# This will be necessary to loop through the entire data\n",
    "non_streaming_dataset = load_dataset(benchmark_dataset, benchmark_task, split='validation', streaming=False)\n",
    "total_num_of_samples = len(non_streaming_dataset)\n",
    "\n",
    "print(f\"The total number of samples in {benchmark_dataset.upper()}-{benchmark_task.upper()} validation is: {total_num_of_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8836b2b7-efa0-4ada-9516-e294d81b0042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KLUE-STS validation dataset...\n",
      "IterableDataset({\n",
      "    features: ['guid', 'source', 'sentence1', 'sentence2', 'labels'],\n",
      "    num_shards: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset *with* streaming\n",
    "print(f\"Loading {benchmark_dataset.upper()}-{benchmark_task.upper()} validation dataset...\")\n",
    "klue_sts_validation = load_dataset(benchmark_dataset, benchmark_task, split='validation', streaming=True)\n",
    "print(klue_sts_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374eecf6-701c-4ea4-a4e3-48a6ecc223ad",
   "metadata": {},
   "source": [
    "## 4. Model Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18dbbea1-3aa4-4e6c-9e22-772a989d5ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd247f1-a7e8-4e0f-abd6-af675ccd3b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "[역할 정의]\n",
    "당신은 두 개의 한국어 문장이 주어졌을 때, 두 문장의 '의미'가 얼마나 유사한지를 판단하는 AI 언어 평가 전문가입니다. \n",
    "문장의 구조나 사용된 단어가 다르더라도, 문맥과 핵심 의미를 파악하여 유사성을 평가해야 합니다.\n",
    "\n",
    "[작업 절차]\n",
    "입력으로 주어진 문장 1과 문장 2의 핵심 의미를 각각 분석합니다. \n",
    "아래 **[평가 기준]**에 따라 두 문장의 의미적 관계를 판단합니다.\n",
    "**[출력 형식]**에 맞춰 결과를 한 줄로 생성합니다.\n",
    "\n",
    "[평가 기준]\n",
    "1. Binary Label (0 또는 1)\n",
    "1 (유사): 두 문장의 핵심 의도나 정보가 사실상 동일하다고 볼 수 있는 경우. 한 문장이 다른 문장의 요약, 부연 설명이거나, 같은 사실을 다른 표현으로 말하는 경우를 포함합니다.\n",
    "0 (다름): 두 문장이 전달하는 핵심 정보나 의도가 명백히 다른 경우. 같은 주제를 다루더라도 초점이 다르거나, 서로 관련이 없는 내용을 말하는 경우는 '다름'으로 판단합니다.\n",
    "\n",
    "2. Real-valued Label (0.0 ~ 5.0)\n",
    "5.0: 완전 동일: 문장 부호, 띄어쓰기, 조사 등 사소한 차이만 있을 뿐, 의미가 100% 동일합니다.\n",
    "예: \"나는 밥을 먹는다\" vs \"나는 밥을 먹어\"\n",
    "\n",
    "4.0 ~ 4.9: 거의 동일: 사용된 어휘나 문장 구조는 다르지만, 전달하는 핵심 정보와 뉘앙스가 완전히 동일합니다.\n",
    "예: \"오늘 날씨가 정말 좋다\" vs \"오늘 날씨가 무척 화창하네\"\n",
    "\n",
    "3.0 ~ 3.9: 대체로 유사: 핵심 정보는 같지만, 부가 정보가 추가되거나 생략되어 약간의 의미 차이가 발생합니다.\n",
    "예: \"나는 아침으로 밥을 먹었다\" vs \"나는 밥을 먹었다\"\n",
    "\n",
    "2.0 ~ 2.9: 주제는 같으나 초점은 다름: 같은 주제나 상황에 대해 이야기하지만, 각 문장이 강조하는 지점이나 전달하는 정보가 다릅니다.\n",
    "예: \"배가 고파서 식당에 갔다\" vs \"그 식당의 김치찌개는 정말 맛있다\"\n",
    "\n",
    "1.0 ~ 1.9: 간접적 연관성만 있음: 공통된 단어가 있거나 소재가 겹치지만, 두 문장이 말하고자 하는 바는 완전히 다릅니다.\n",
    "예: \"나는 어제 축구를 봤다\" vs \"손흥민은 대단한 축구 선수다\"\n",
    "\n",
    "0.0 ~ 0.9: 전혀 관련 없음: 두 문장 사이에 어떠한 의미적 연관성도 찾을 수 없습니다.\n",
    "예: \"내일 회의는 3시에 시작합니다\" vs \"고양이는 귀여운 동물이다\"\n",
    "\n",
    "[출력 형식]\n",
    "binary-label 값과 real-label 값을 쉼표(,)로 구분하여 한 줄에 출력합니다.\n",
    "형식: binary-label: [값], real-label: [값]\n",
    "\n",
    "[예시]\n",
    "입력:\n",
    "문장1: \"코로나19의 전 세계적 유행으로 인해 해외여행이 어려워졌다.\"\n",
    "문장2: \"펜데믹 상황 때문에 사람들이 국외로 나가는 것이 힘들어졌다.\"\n",
    "출력:\n",
    "binary-label: 1, real-label: 4.5\n",
    "\n",
    "입력:\n",
    "문장1: \"이 영화 정말 재미있더라.\"\n",
    "문장2: \"그 영화 주인공 연기가 인상 깊었어.\"\n",
    "출력:\n",
    "binary-label: 0, real-label: 2.8\n",
    "\n",
    "입력:\n",
    "문장1: \"노트북 배터리가 거의 다 닳았네.\"\n",
    "문장2: \"오늘 저녁 메뉴는 뭘로 할까?\"\n",
    "출력:\n",
    "binary-label: 0, real-label: 0.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891be4ba-6ae3-4363-babe-c7ce0c358777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt(sentence1, sentence2):\n",
    "    prompt = f\"\"\"\n",
    "문장1: {sentence1}\n",
    "문장2: {sentence2} \n",
    "\"\"\"\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98663024-3040-4d46-8c7c-9e60620a86d1",
   "metadata": {},
   "source": [
    "## 5. Test with a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495f765d-274f-4ff2-9e2c-c4b30d3f0f53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장1: 무엇보다도 호스트분들이 너무 친절하셨습니다.\n",
      "문장2: 무엇보다도, 호스트들은 매우 친절했습니다.\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(klue_sts_validation))\n",
    "sample_prompt = create_prompt( sample['sentence1'], sample['sentence2'] )\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e368f3f-d4ef-4559-83d9-f6ce5962a905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = sample_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b62cab5-2bd6-4a12-94b1-a27829146f66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "binary-label: 1, real-label: 5.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.0,  # 0 for consistency\n",
    "        system_instruction=system_instruction,\n",
    "        #top_p=0.95,\n",
    "        #candidate_count=1,\n",
    "        #thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94456dc-3fb7-4456-98c7-1aa3f7b46d56",
   "metadata": {},
   "source": [
    "## 6. Test the loop with only ten samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94893877-6774-405f-8757-750118d0c159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from tqdm import tqdm  # Show the progress\n",
    "import itertools # Import itertools to safely slice the dataset\n",
    "\n",
    "# Initialize variables\n",
    "binary_predictions = []\n",
    "real_predictions   = []\n",
    "true_binary_labels = []\n",
    "true_real_labels   = []\n",
    "# To store sentences for the results table\n",
    "true_sentences1    = [] \n",
    "true_sentences2    = []\n",
    "\n",
    "error_count = 0\n",
    "num_test_samples = 10\n",
    "sleep_interval_between_api_calls = 0.03 # sec\n",
    "#description = f\"Evaluating with {MODEL_ID}\"\n",
    "description = f\"Evaluating {MODEL_ID} on {num_test_samples} samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89c1c2b0-86cc-4faa-b5d2-3dce3e3b11fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gemini-2.5-flash on 10 samples: 100%|██████████| 10/10 [01:05<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Finished.\n",
      "Total samples processed: 10\n",
      "Successful predictions: 10\n",
      "Format errors or API issues: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop\n",
    "for i, sample in enumerate(tqdm(itertools.islice(klue_sts_validation, num_test_samples), desc=description, total=num_test_samples), 1):\n",
    "    sentence1 = sample['sentence1']\n",
    "    sentence2 = sample['sentence2']\n",
    "    sample_prompt = create_prompt(sentence1, sentence2)\n",
    "\n",
    "    ground_truth_binary = sample['labels']['binary-label']\n",
    "    ground_truth_real = sample['labels']['real-label']\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=sample_prompt,\n",
    "            config=GenerateContentConfig(\n",
    "                temperature=0.0,  # 0 for consistency\n",
    "                system_instruction=system_instruction,\n",
    "            ),\n",
    "        )\n",
    "        model_output = response.text.strip()\n",
    "\n",
    "        # Parse model_output with regular expression\n",
    "        match = re.search(r\"binary-label:\\s*([01])\\s*,\\s*real-label:\\s*([0-9.]+)\", model_output)\n",
    "\n",
    "        if match:\n",
    "            # Extract values and convert types\n",
    "            b_pred = int(match.group(1))\n",
    "            r_pred = float(match.group(2))\n",
    "            \n",
    "            # Append results to lists\n",
    "            binary_predictions.append(b_pred)\n",
    "            real_predictions.append(r_pred)\n",
    "            true_binary_labels.append(ground_truth_binary)\n",
    "            true_real_labels.append(ground_truth_real)\n",
    "            # Store sentences\n",
    "            true_sentences1.append(sentence1)\n",
    "            true_sentences2.append(sentence2)\n",
    "\n",
    "        else:\n",
    "            error_count += 1\n",
    "            print(f\"\\n----- Sample {i}/{num_test_samples} (Format Error) -----\")\n",
    "            print(f\"Mismatched model output: {model_output}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        error_count += 1\n",
    "        print(f\"\\n Sample {i}/{num_test_samples} (API Error)\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # To prevent overloading the API\n",
    "    time.sleep( sleep_interval_between_api_calls )\n",
    "\n",
    "print(f\"\\nEvaluation Finished.\")\n",
    "print(f\"Total samples processed: {len(binary_predictions) + error_count}\")\n",
    "print(f\"Successful predictions: {len(binary_predictions)}\")\n",
    "print(f\"Format errors or API issues: {error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6914f-8047-4b7b-84f2-619f08535dbc",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "- For binary-labels, calculate the classification_report. \n",
    "- For real-labels, calculate regression metrics such as mean squared error (MSE) and mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35725163-219e-4edb-af70-864a1e554008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Sample-by-Sample Comparison Table =====\n",
      "\n",
      "====================== Sample 1/10 ======================\n",
      "Sentence 1: 무엇보다도 호스트분들이 너무 친절하셨습니다.\n",
      "Sentence 2: 무엇보다도, 호스트들은 매우 친절했습니다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=1, Real=4.86\n",
      "  - Prediction   : Binary=1, Real=5.00\n",
      "\n",
      "====================== Sample 2/10 ======================\n",
      "Sentence 1: 주요 관광지 모두 걸어서 이동가능합니다.\n",
      "Sentence 2: 위치는 피렌체 중심가까지 걸어서 이동 가능합니다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=1.43\n",
      "  - Prediction   : Binary=0, Real=2.50\n",
      "\n",
      "====================== Sample 3/10 ======================\n",
      "Sentence 1: 학생들의 균형 있는 영어능력을 향상시킬 수 있는 학교 수업을 유도하기 위해 2018학년도 수능부터 도입된 영어 영역 절대평가는 올해도 유지한다.\n",
      "Sentence 2: 영어 영역의 경우 학생들이 한글 해석본을 암기하는 문제를 해소하기 위해 2016학년도부터 적용했던 EBS 연계 방식을 올해도 유지한다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=1.29\n",
      "  - Prediction   : Binary=0, Real=2.50\n",
      "\n",
      "====================== Sample 4/10 ======================\n",
      "Sentence 1: 다만, 도로와 인접해서 거리의 소음이 들려요.\n",
      "Sentence 2: 하지만, 길과 가깝기 때문에 거리의 소음을 들을 수 있습니다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=1, Real=3.71\n",
      "  - Prediction   : Binary=1, Real=4.50\n",
      "\n",
      "====================== Sample 5/10 ======================\n",
      "Sentence 1: 형이 다시 캐나다 들어가야 하니 가족모임 일정은 바꾸지 마세요.\n",
      "Sentence 2: 가족 모임 일정은 바꾸지 말도록 하십시오.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=2.50\n",
      "  - Prediction   : Binary=1, Real=3.50\n",
      "\n",
      "====================== Sample 6/10 ======================\n",
      "Sentence 1: 방안에 필요한 시설이 모두 있어서 매우 편리합니다.\n",
      "Sentence 2: 특히, 숙소 근처에 안전한 실내 주차장이 있어서 편리합니다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=1.14\n",
      "  - Prediction   : Binary=0, Real=2.50\n",
      "\n",
      "====================== Sample 7/10 ======================\n",
      "Sentence 1: 최근 국민들의 여행심리 위축 등으로 동남아 등 다른 노선까지 영향을 받는 상황이다.\n",
      "Sentence 2: 동남아시아와 같은 다른 노선은 최근 사람들의 여행 감정의 하락에 영향을 받았습니다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=1, Real=3.60\n",
      "  - Prediction   : Binary=1, Real=4.50\n",
      "\n",
      "====================== Sample 8/10 ======================\n",
      "Sentence 1: 또한 지원에 필요한 소득과 재산 심사 대상을 본인과 배우자로 완화했다.\n",
      "Sentence 2: 국토부는 여기에 소득 기준을 더욱 완화했다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=1.29\n",
      "  - Prediction   : Binary=0, Real=2.50\n",
      "\n",
      "====================== Sample 9/10 ======================\n",
      "Sentence 1: 일본종교문화를 잘 이해하시는 분이 이용하시는 게 좋아요\n",
      "Sentence 2: 그리고 슬리퍼를 하나 준비하시는 게 좋아요\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=0.00\n",
      "  - Prediction   : Binary=0, Real=0.00\n",
      "\n",
      "====================== Sample 10/10 ======================\n",
      "Sentence 1: 다음에 도쿄를 또 간다면 무조건 이 곳을 다시 찾을 겁니다.\n",
      "Sentence 2: 독일을 다음에 또간다면 이숙소에 머물겁니다.\n",
      "---------------------------------------------------------\n",
      "  - Ground Truth : Binary=0, Real=2.20\n",
      "  - Prediction   : Binary=1, Real=4.50\n",
      "==========================================================\n",
      "\n",
      "===== Display Evaluation Metrics =====\n",
      "\n",
      "\n",
      "--- gemini-2.5-flash KLUE-STS (Zero-shot) Benchmark Results ---\n",
      "Evaluated on 10 samples.\n",
      "\n",
      "===== Binary Label (Classification) =====\n",
      "\n",
      "Overall Accuracy: 0.8000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Different (0)       1.00      0.71      0.83         7\n",
      "  Similar (1)       0.60      1.00      0.75         3\n",
      "\n",
      "     accuracy                           0.80        10\n",
      "    macro avg       0.80      0.86      0.79        10\n",
      " weighted avg       0.88      0.80      0.81        10\n",
      "\n",
      "\n",
      "===== Real Label (Regression) =====\n",
      "\n",
      "Mean Squared Error (MSE): 1.3677\n",
      "Mean Absolute Error (MAE): 0.9986\n",
      "Pearson Correlation: 0.9082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # To create the results table\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# --- Assume these lists are populated from the previous evaluation loop ---\n",
    "# For demonstration purposes, let's create some dummy data.\n",
    "# In your actual code, these lists will be filled by the loop.\n",
    "# true_sentences1 = ['Sentence 1A', 'Sentence 1B'] \n",
    "# true_sentences2 = ['Sentence 2A', 'Sentence 2B']\n",
    "# true_binary_labels = [1, 0]\n",
    "# true_real_labels = [4.5, 1.2]\n",
    "# binary_predictions = [1, 1]\n",
    "# real_predictions = [4.2, 2.0]\n",
    "\n",
    "print(\"\\n===== Sample-by-Sample Comparison Table =====\")\n",
    "\n",
    "if true_binary_labels and binary_predictions:\n",
    "    # Create a dictionary with the results\n",
    "    results_data = {\n",
    "        'Sentence 1': [s1 for s1, s2 in zip(true_sentences1, true_sentences2)],\n",
    "        'Sentence 2': [s2 for s1, s2 in zip(true_sentences1, true_sentences2)],\n",
    "        'True Binary': true_binary_labels,\n",
    "        'Pred Binary': binary_predictions,\n",
    "        'True Real': [f\"{x:.2f}\" for x in true_real_labels],\n",
    "        'Pred Real': [f\"{x:.2f}\" for x in real_predictions]\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Create and display the pandas DataFrame\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Set display options to show full text in columns\n",
    "    #pd.set_option('display.max_colwidth', None)\n",
    "    #pd.set_option('display.width', 1000)\n",
    "    #print(results_df.to_string())\n",
    "    # -> The output table looks messy!\n",
    "    \n",
    "    # Iterate over the DataFrame and print each sample in a structured, readable format\n",
    "    for index, row in results_df.iterrows():\n",
    "        print(f\"\\nSample {index + 1}/{len(results_df)}\")\n",
    "        print(f\"Sentence 1: {row['Sentence 1']}\")\n",
    "        print(f\"Sentence 2: {row['Sentence 2']}\")\n",
    "        print(\"---------------------------------------------------------\")\n",
    "        print(f\"  - Ground Truth : Binary={row['True Binary']}, Real={row['True Real']}\")\n",
    "        print(f\"  - Prediction   : Binary={row['Pred Binary']}, Real={row['Pred Real']}\")\n",
    "    print(\"==========================================================\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo valid predictions to display in the results table.\")\n",
    "\n",
    "print(\"\\n===== Display Evaluation Metrics =====\")\n",
    "print(f\"\\n\\n--- {MODEL_ID} KLUE-STS (Zero-shot) Benchmark Results ---\")\n",
    "print(f\"Evaluated on {len(true_binary_labels)} samples.\")\n",
    "\n",
    "print(\"\\n===== Binary Label (Classification) =====\")\n",
    "if true_binary_labels and binary_predictions:\n",
    "    accuracy = accuracy_score(true_binary_labels, binary_predictions)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    report = classification_report(\n",
    "        true_binary_labels,\n",
    "        binary_predictions,\n",
    "        target_names=['Different (0)', 'Similar (1)'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"\\nCould not calculate classification metrics. No valid binary predictions found.\")\n",
    "\n",
    "    \n",
    "print(\"\\n===== Real Label (Regression) =====\")\n",
    "if true_real_labels and real_predictions:\n",
    "    mse = mean_squared_error(true_real_labels, real_predictions)\n",
    "    print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "    mae = mean_absolute_error(true_real_labels, real_predictions)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "    pearson_corr, _ = pearsonr(true_real_labels, real_predictions)\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate regression metrics. No valid real-valued predictions found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a925baf-e28a-49b7-a8e4-3f5e342b3d6d",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "### Metrics for Binary Classification\n",
    "- Accuracy: The proportion of total samples for which the model correctly predicted 'similar (1)' or 'different (0)'.\n",
    "- F1-Score: The harmonic mean of Precision and Recall. \n",
    "  - It is a reliable classification performance metric, even when the data is imbalanced. \n",
    "  - The F1-score for the \"similar (1)\" class is typically used as the key metric.\n",
    "\n",
    "### Metrics for Real-valued Regression Metrics\n",
    "- RMSE (Root Mean Squared Error)\n",
    "  - The average magnitude of the error between the model's predicted values and the actual values. \n",
    "  - A value closer to 0 signifies that the model has accurately predicted the fine-grained scores between 0.0 and 5.0. \n",
    "  - This metric is sensitive to outliers.\n",
    "\n",
    "- MAE (Mean Absolute Error)\n",
    "  - The average of the absolute errors. \n",
    "  - It is less sensitive to outliers than RMSE and is useful for intuitively interpreting the actual magnitude of the error. \n",
    "  - For example, an MAE of 0.5 can be understood as the model having an average error of approximately ±0.5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46885c34-a1ae-4dfc-8c54-2017a9393a2f",
   "metadata": {},
   "source": [
    "## 8. Loop through all the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84701d1a-d12c-4295-a1ee-6c03a726e354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gemini-2.5-flash with the entire dataset of (519 samples)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Configure output files\n",
    "num_random_samples_to_save = 100 # for quick review\n",
    "random_samples_filename = f\"{benchmark_dataset}-{benchmark_task}-{MODEL_ID}-random_samples_for_review.txt\"\n",
    "full_results_filename = f\"{benchmark_dataset}-{benchmark_task}-{MODEL_ID}-full_evaluation_results.csv\"\n",
    "\n",
    "# Initialize variables\n",
    "binary_predictions = []\n",
    "real_predictions   = []\n",
    "true_binary_labels = []\n",
    "true_real_labels   = []\n",
    "# To store sentences for the results table\n",
    "true_sentences1    = [] \n",
    "true_sentences2    = []\n",
    "\n",
    "error_count = 0\n",
    "sleep_interval_between_api_calls = 0.03 # sec\n",
    "\n",
    "# Get the total number of samples\n",
    "try:\n",
    "    # total_num_of_samples was computed at the beginning BEFORE loading the dataset in the streaming mode\n",
    "    # Note: len(klue_sts_validation) will fail with \"TypeError: object of type 'IterableDataset' has no len()\"\n",
    "    description = f\"Evaluating {MODEL_ID} with the entire dataset of ({total_num_of_samples} samples)\"\n",
    "except TypeError:\n",
    "    # Fallback for datasets that don't have a __len__ method\n",
    "    total_num_of_samples = None\n",
    "    description = f\"Evaluating {MODEL_ID} on full KLUE-STS dataset\"\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3be028b8-3e9d-4a8b-a35d-1957af59f67e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the full validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gemini-2.5-flash with the entire dataset of (519 samples): 100%|██████████| 519/519 [1:03:10<00:00,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Finished.\n",
      "Total samples processed: 519\n",
      "Successful predictions: 519\n",
      "Format errors or API issues: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main evaluation loop\n",
    "print(\"Processing the full validation dataset...\")\n",
    "for i, sample in enumerate(tqdm(klue_sts_validation, desc=description, total=total_num_of_samples), 1):\n",
    "    sentence1 = sample['sentence1']\n",
    "    sentence2 = sample['sentence2']\n",
    "    sample_prompt = create_prompt(sentence1, sentence2)\n",
    "\n",
    "    ground_truth_binary = sample['labels']['binary-label']\n",
    "    ground_truth_real = sample['labels']['real-label']\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=sample_prompt,\n",
    "            config=GenerateContentConfig(\n",
    "                temperature=0.0, # 0 for consistency\n",
    "                system_instruction=system_instruction,\n",
    "            ),\n",
    "        )\n",
    "        model_output = response.text.strip()\n",
    "\n",
    "        match = re.search(r\"binary-label:\\s*([01])\\s*,\\s*real-label:\\s*([0-9.]+)\", model_output)\n",
    "\n",
    "        if match:\n",
    "            b_pred = int(match.group(1))\n",
    "            r_pred = float(match.group(2))\n",
    "            \n",
    "            binary_predictions.append(b_pred)\n",
    "            real_predictions.append(r_pred)\n",
    "            true_binary_labels.append(ground_truth_binary)\n",
    "            true_real_labels.append(ground_truth_real)\n",
    "            true_sentences1.append(sentence1) \n",
    "            true_sentences2.append(sentence2)    \n",
    "        else:\n",
    "            error_count += 1\n",
    "            print(f\"\\n Sample {i}/{total_num_of_samples} (Format Error)\")\n",
    "            print(f\"Mismatched model output: {model_output}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"\\n Sample {i}/{total_num_of_samples} (API Error)\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    time.sleep(sleep_interval_between_api_calls)\n",
    "\n",
    "print(f\"\\nEvaluation Finished.\")\n",
    "print(f\"Total samples processed: {len(binary_predictions) + error_count}\")\n",
    "print(f\"Successful predictions: {len(binary_predictions)}\")\n",
    "print(f\"Format errors or API issues: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb0513cc-cd85-44ac-a6a9-e4722bf8e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved all 519 results to 'full_evaluation_results.csv'\n",
      "Successfully saved 100 random samples to 'random_samples_for_review.txt'\n",
      "\n",
      "\n",
      " gemini-2.5-flash KLUE-STS (Zero-shot) Benchmark Results\n",
      "\n",
      "===== Binary Label (Classification) =====\n",
      "\n",
      "Overall Accuracy: 0.8439\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Different (0)       0.95      0.77      0.85       299\n",
      "  Similar (1)       0.75      0.95      0.84       220\n",
      "\n",
      "     accuracy                           0.84       519\n",
      "    macro avg       0.85      0.86      0.84       519\n",
      " weighted avg       0.87      0.84      0.84       519\n",
      "\n",
      "\n",
      "===== Real Label (Regression) =====\n",
      "\n",
      "Mean Squared Error (MSE): 1.5102\n",
      "Mean Absolute Error (MAE): 0.9819\n",
      "Pearson Correlation: 0.8116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Create the full results DataFrame\n",
    "if true_binary_labels and binary_predictions:\n",
    "    results_data = {\n",
    "        'Sentence 1': true_sentences1,\n",
    "        'Sentence 2': true_sentences2,\n",
    "        'True Binary': true_binary_labels,\n",
    "        'Pred Binary': binary_predictions,\n",
    "        'True Real': true_real_labels,\n",
    "        'Pred Real': real_predictions\n",
    "    }\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "\n",
    "    # 1. Save ALL results to a CSV file for later analysis\n",
    "    try:\n",
    "        results_df.to_csv(full_results_filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nSuccessfully saved all {len(results_df)} results to '{full_results_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving full results to CSV: {e}\")\n",
    "\n",
    "    # 2. Save a random selection of samples to a text file for quick review\n",
    "    if not results_df.empty and num_random_samples_to_save > 0:\n",
    "        try:\n",
    "            num_to_sample = min(num_random_samples_to_save, len(results_df))\n",
    "            random_samples_df = results_df.sample(n=num_to_sample)\n",
    "            \n",
    "            with open(random_samples_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"--- Randomly Selected Samples for Review ({num_to_sample} of {len(results_df)}) ---\\n\")\n",
    "                for index, row in random_samples_df.iterrows():\n",
    "                    f.write(f\"\\n====================== Sample (Original Index: {index}) ======================\\n\")\n",
    "                    f.write(f\"Sentence 1: {row['Sentence 1']}\\n\")\n",
    "                    f.write(f\"Sentence 2: {row['Sentence 2']}\\n\")\n",
    "                    f.write(\"---------------------------------------------------------\\n\")\n",
    "                    f.write(f\"  - Ground Truth : Binary={row['True Binary']}, Real={row['True Real']:.2f}\\n\")\n",
    "                    f.write(f\"  - Prediction   : Binary={row['Pred Binary']}, Real={row['Pred Real']:.2f}\\n\")\n",
    "                f.write(\"\\n==========================================================\\n\")\n",
    "            print(f\"Successfully saved {num_to_sample} random samples to '{random_samples_filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving random samples to text file: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo valid predictions were generated to save or analyze.\")\n",
    "\n",
    "\n",
    "# 3. Display Final Evaluation Metrics\n",
    "print(f\"\\n\\n {MODEL_ID} KLUE-STS (Zero-shot) Benchmark Results\")\n",
    "\n",
    "print(\"\\n===== Binary Label (Classification) =====\")\n",
    "if true_binary_labels and binary_predictions:\n",
    "    accuracy = accuracy_score(true_binary_labels, binary_predictions)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    report = classification_report(\n",
    "        true_binary_labels,\n",
    "        binary_predictions,\n",
    "        target_names=['Different (0)', 'Similar (1)'],\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"\\nCould not calculate classification metrics. No valid binary predictions found.\")\n",
    "\n",
    "print(\"\\n===== Real Label (Regression) =====\")\n",
    "if true_real_labels and real_predictions:\n",
    "    mse = mean_squared_error(true_real_labels, real_predictions)\n",
    "    print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "    mae = mean_absolute_error(true_real_labels, real_predictions)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "    pearson_corr, _ = pearsonr(true_real_labels, real_predictions)\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not calculate regression metrics. No valid real-valued predictions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951d311-81a4-4f69-b62a-dd143a690355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
